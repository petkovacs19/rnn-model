{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate the power of recurrent neural networks in learning a character level language model.\n",
    "\n",
    "In the example below we  use a list of dutch cities as input and we  generate new city names by learning the character level patterns in the existing names. The model generates new sequences of characters using the patterns in the input sequence.\n",
    "\n",
    "\n",
    "We will use tensorflow to create the model. The below architecture can be used to learn the patterns in other sequences too. Feel free to experiment with your own dataset and modify the architecture of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Recurrent Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network is a neural network that has an internal state/memory. This internal state stores information on the previous examples the cell has seen, and may affect the outcome the cell will output at the next timestep. As the internal state updated through the training, RNNs are very efficient in learning patterns in sequences of data. RNNs are often used when the order of the values in the dataset matters and has importance in finding a value of a given sample. \n",
    "\n",
    "A classic use case  is a timeseries of stock market data. A naive example is an indicator that gives a prediction whether a value of a stock will go up or down based on the current change of the stock value and changes in  the past. This means the indicator needs to be able to define long term patterns in the sequence. \n",
    "\n",
    "Another popular use case is sentiment analysis. In order to define the sentiment of a sentence(positive, negative, neutral) it is essential to learn dependencies between the words. e.g.: \"I am happy\" - \"I am not happy\". The word \"happy\" has a positive sentiment but used with the word \"not\" makes it negative.\n",
    "\n",
    "Sequence to sequence transformations such as language translation and speech recognition are other examples where RNNs have been applied with a great success.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below represents an abstract overview of how an RNN works. At timestep t X<sub>t</sub> is fed into the network that will output  some h<sub>t</sub> and also update its internal states based on the value it has seen. We move on to timestep t+1 and the process is repeated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"10%\" src=\"rnn_rolled.png\"></img>\n",
    "<caption>Recurrent Neural Network src: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below is visualisation of an RNN network unrolled. A basic RNN cell has two inputs:\n",
    "* X<sub>t</sub> - our data at timestep t\n",
    "* S<sub>t-1</sub> - the state of the network at timestep t-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn_unrolled.png\"></img>\n",
    "<caption>Recurrent Neural Network src: http://colah.github.io/posts/2015-08-Understanding-LSTMs/</caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we will be working with an LSTM(Long Short Term Memory)  cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Long Short Term Memory has the following inputs:\n",
    "    * Hidden state: hidden cell state of the RNN at timestep t-1\n",
    "$$ a^{\\langle t-1 \\rangle} $$\n",
    "    * Cell state: cell state of the RNN at timestep t-1\n",
    "$$ c^{\\langle t-1 \\rangle} $$\n",
    "the following gates:\n",
    "    * Forget gate: how much of the cell state coming from the previous time step the network will ignore\n",
    "$$\\Gamma_f^{\\langle t \\rangle} = \\sigma(W_f[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_f)\\tag{1} $$\n",
    "    * Keep gate: how much of the new proposed cell state the network will keep\n",
    "$$\\Gamma_u^{\\langle t \\rangle} = \\sigma(W_u[a^{\\langle t-1 \\rangle}, x^{\\{t\\}}] + b_u)\\tag{2} $$\n",
    "    * Output gate: how much of the calculated output will be returned\n",
    "$$ \\Gamma_o^{\\langle t \\rangle}=  \\sigma(W_o[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_o)\\tag{5}$$ \n",
    "performs the following calculations:\n",
    "    * Proposed cell state: calculated from the hidden state of the previous step and x\n",
    "$$ \\tilde{c}^{\\langle t \\rangle} = \\tanh(W_c[a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}] + b_c)\\tag{3} $$\n",
    "    * New cell state: element wise product of forget gate and the last cell state +  element wise product of forget gate and proposed cell state\n",
    "$$ c^{\\langle t \\rangle} = \\Gamma_f^{\\langle t \\rangle}* c^{\\langle t-1 \\rangle} + \\Gamma_u^{\\langle t \\rangle} *\\tilde{c}^{\\langle t \\rangle} \\tag{4} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a good understanding of how LSTM works I would recommend <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"> colah's blog.</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pkovacs/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Reading input dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the dataset, and count the number of characters that will be our vocabulary size. The goal is to train an RNN network that given an input sequence (t, t+1, t+2.... t+n) will return a probability distribution for the value of t+n+1. We consider every character as a class and our neural network will be optimized to classify an input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'nl_cities.csv'\n",
    "data = open(file, 'r').read().lower()\n",
    "chars = list(set(data))\n",
    "num_classes = vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file) as f:\n",
    "    datarows = f.readlines()\n",
    "datarows = [x.lower().strip() for x in datarows]\n",
    "np.random.shuffle(datarows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in dataset 38\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters in dataset\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a map from a character to a class id, and a map from a class id to a character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network will take one-hot encodings as an input at every time step. A one-hot encoding is a representation that makes training of the network easier.\n",
    "Imagine there are four classes: {0, 1, 2, 3} and our dataset is the following: <br>\n",
    "2 <br>\n",
    "2 <br>\n",
    "1 <br>\n",
    "3 <br>\n",
    "Their one-hot encoding would like the following: <br>\n",
    "[0, 0, 1, 0] <br>\n",
    "[0, 0, 1, 0] <br>\n",
    "[0, 1, 0, 0] <br>\n",
    "[0, 0, 0, 1] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two dictionaries to help us during the implementation: </br>\n",
    "* a dictionary from class_id to character\n",
    "* and a dictionary from character to class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_class_id = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "class_id_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow first you define the graph of your model then you execute it by running it in a session.\n",
    "When working with tensorflow I would recommend the following steps:\n",
    "1. Define your input and target placeholders.\n",
    "2. Build up the graph of your model.\n",
    "3. Define a cost function that will measure how well your model performs against your objectives\n",
    "4. Define an optimizer to minimize your cost function\n",
    "5. Execute your model by feeding in the input data and target data to the placeholders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create placeholders for input and target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensorflow placeholders are used to define data sources to your graph.\n",
    "Our graph model will have two datasource:\n",
    "    * X - a number of sequences with a maximum length of max_timesteps\n",
    "    * Y - same as X shifted by to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(length, batch_size=1):\n",
    "    \"\"\"\n",
    "    Creates the placeholder for the input date and the target data.\n",
    "    batch_size - Number of data rows, default to 1\n",
    "    max_timesteps - The maximum length of sequence\n",
    "    \n",
    "    return X,Y placeholders\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.int32, [batch_size, length])\n",
    "    Y = tf.placeholder(tf.int32, [batch_size, length])\n",
    "    \n",
    "    return X,Y    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model - forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(X, Y, state_size, num_classes, batch_size):\n",
    "    \"\"\"\n",
    "    Constructs the graph of the model\n",
    "    Creates the placeholder for the hidden state and cell state\n",
    "    batch_size - Number of data rows\n",
    "    state_size - state_size of the RNN Unit\n",
    "    num_classes - number of classes we are predicting\n",
    "    parameters - matrices containing weights\n",
    "    \n",
    "    return cell_state, hidden_state, current_state, predictions, total_loss\n",
    "    \"\"\"\n",
    "    #Create one hot representaion from input X placeholder\n",
    "    inputs_series = tf.one_hot(X, num_classes)\n",
    "    \n",
    "    #Create lstm_cell\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(state_size, state_is_tuple=True)\n",
    "    \n",
    "    #Create placeholder for cell state and hidden state\n",
    "    cell_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "    hidden_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
    "    #LSTMStateTuple represent the state of the cell\n",
    "    rnn_tuple_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)\n",
    "    \n",
    "    #Unroll the cell to a max_length connected rnn cells\n",
    "    #The length of the timeseries is dynamically determined during runtime, as every datarow has different length\n",
    "    outputs, current_state = tf.nn.dynamic_rnn(lstm_cell,\n",
    "                                               inputs_series,\n",
    "                                               initial_state=rnn_tuple_state,dtype=tf.float32)\n",
    "    #Determine length of sequence\n",
    "    length = tf.shape(X)[1]\n",
    "    \n",
    "    #outputs will have a shape of batch_size X state_size\n",
    "    #we define and out matrix of shape state_size, num_classes\n",
    "    # outputs * out_weight will result in an output of the desired shape\n",
    "    out_weight = tf.get_variable('out_weight', [state_size, num_classes])\n",
    "    out_bias = tf.get_variable('out_bias', [num_classes])\n",
    "\n",
    "\n",
    "    logits = tf.reshape(tf.matmul(tf.reshape(outputs, [-1, state_size]), out_weight) + out_bias,[batch_size, length, num_classes])\n",
    "    \n",
    "    #Create prediction for sampling purposes\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    #Calculate loss\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    #Calculate total loss as average over timesteps\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    \n",
    "    return cell_state, hidden_state, current_state, predictions, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(training_steps=35000, learning_rate=0.05, state_size=5, sample_size=10, every=2000, batch_size=1):\n",
    "    assert batch_size == 1, \"batch_size greater then 1 not supported yet\"\n",
    "    # Get placeholders for X and Y\n",
    "    # We will leave max_timesteps undefined, it will be evaluated during runtime\n",
    "    X, Y = create_placeholders(None)\n",
    "    #\n",
    "    cell_state, hidden_state, current_state, predictions, total_loss = create_graph(X, Y, state_size, num_classes, batch_size)\n",
    "    \n",
    "    #Define train step\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    with tf.Session() as sess:\n",
    "        #Initialize variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)        \n",
    "        \n",
    "        for step in range(1, training_steps+1):\n",
    "            #Zero Initialize the hidden and cell state of the lstm\n",
    "            _current_state = np.zeros((2, batch_size, state_size))\n",
    "            row_index = step % len(datarows)\n",
    "\n",
    "            X_train = [-1] + [char_to_class_id[ch] for ch in datarows[row_index]] \n",
    "            Y_train = X_train[1:] + [char_to_class_id[\"\\n\"]]\n",
    "\n",
    "            # Reshape data to get 1x28 shaped element\n",
    "            batch_x = np.expand_dims(np.array(X_train), axis=0)\n",
    "            batch_y = np.expand_dims(np.array(Y_train), axis=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            cost, _current_state, _ = sess.run([total_loss, current_state, train_step],\n",
    "                                                           feed_dict={\n",
    "                                                                       X: batch_x,\n",
    "                                                                       Y: batch_y,\n",
    "                                                                       cell_state: _current_state[0],\n",
    "                                                                       hidden_state: _current_state[1]})\n",
    "                \n",
    "            #Print Loss and sample from trained grapd\n",
    "            if step % every == 0:\n",
    "                print(\"Step \" + str(step) + \", Loss= \" + \\\n",
    "                      \"{:.4f}\".format(cost))\n",
    "                print(\"list of sampled characters:\")\n",
    "                for sample in range(sample_size):\n",
    "                    _current_sample_state = np.zeros((2, batch_size, state_size))\n",
    "                    #sample a prediction\n",
    "                    idx = -1\n",
    "                    newline_character = char_to_class_id['\\n']\n",
    "                    counter = 0\n",
    "                    indices = []\n",
    "                    X_eval = [-1]\n",
    "                    X_eval = np.expand_dims(np.array(X_eval), axis=0)\n",
    "                    while (idx != newline_character and counter != 50):\n",
    "#                         np.random.seed(counter+sample) \n",
    "                        pred_out, _current_sample_state = sess.run([predictions, current_state],\n",
    "                                                                   feed_dict={\n",
    "                                                                    X: X_eval, \n",
    "                                                                    cell_state: _current_sample_state[0],\n",
    "                                                                    hidden_state: _current_sample_state[1]})\n",
    "                        pred_probs = pred_out[0][0]\n",
    "\n",
    "                        #Sample a character using the output probability distribution\n",
    "                        idx = np.random.choice(np.arange(0,vocab_size), p = pred_out.ravel())\n",
    "                        #Append sampled character to a list\n",
    "                        character = class_id_to_char[idx]\n",
    "                        indices.append(idx)\n",
    "                        #set sampled characted as an input in the next timestep\n",
    "                        X_eval = [idx]\n",
    "                        X_eval = np.expand_dims(np.array(X_eval), axis=0)\n",
    "                        counter += 1\n",
    "                    print(''.join([class_id_to_char[i] for i in indices]).strip())\n",
    "                \n",
    "        print(\"Optimization Finished!\")\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Loss= 2.0457\n",
      "list of sampled characters:\n",
      "hoindek\n",
      "dekhejkhi kuen\n",
      "erd\n",
      "hapssendem\n",
      "decan a l-th\n",
      "ercgensvelkerl\n",
      "ar\n",
      "hoodrerdoo\n",
      "len\n",
      "haldrae\n",
      "Step 4000, Loss= 2.5095\n",
      "list of sampled characters:\n",
      "delend\n",
      "blosigmesgile\n",
      "enci\n",
      "glonf\n",
      "has\n",
      "vered\n",
      "loikeno\n",
      "auken\n",
      "-gleoik\n",
      "galdawold\n",
      "Step 6000, Loss= 2.3525\n",
      "list of sampled characters:\n",
      "frssgesj\n",
      "boi-scsek\n",
      "kormhorp\n",
      "albkerl\n",
      "mayoert\n",
      "gulmengessayk\n",
      "hhasnfel\n",
      "bynlatebde\n",
      "buin\n",
      "kuixeresdg\n",
      "Step 8000, Loss= 2.0971\n",
      "list of sampled characters:\n",
      "de mom\n",
      "hoorda\n",
      "hi id\n",
      "hauf\n",
      "dejgheene\n",
      "lom\n",
      "kruold\n",
      "veakral\n",
      "boanhoeke\n",
      "jhtwi\n",
      "Step 10000, Loss= 2.3584\n",
      "list of sampled characters:\n",
      "brum\n",
      "gesgake\n",
      "\n",
      "burdurke\n",
      "bejsdi driz\n",
      "vrort\n",
      "gruoajs\n",
      "lorghas\n",
      "boezaen\n",
      "horvel\n",
      "Step 12000, Loss= 1.8283\n",
      "list of sampled characters:\n",
      "dendroog\n",
      "kindep\n",
      "ateijg\n",
      "bar\n",
      "boepeg\n",
      "lorlupen\n",
      "kual\n",
      "bagechpeser\n",
      "deunejkerst\n",
      "wend\n",
      "Step 14000, Loss= 2.1790\n",
      "list of sampled characters:\n",
      "doolken\n",
      "lostoind\n",
      "i\n",
      "loubloek\n",
      "bu\n",
      "abose\n",
      "londe\n",
      "gougencr\n",
      "blalden\n",
      "hom\n",
      "Step 16000, Loss= 2.5265\n",
      "list of sampled characters:\n",
      "birk\n",
      "krord\n",
      "rbe de lneum\n",
      "labhavuss hoegen\n",
      "lilt\n",
      "i aene sk\n",
      "krrassar\n",
      "kreilup\n",
      "llerde\n",
      "lantsnin\n",
      "Step 18000, Loss= 1.6399\n",
      "list of sampled characters:\n",
      "heing\n",
      "rirdut\n",
      "derneen\n",
      "klot ankkiiul-kroibelnuel\n",
      "bamlberkumef\n",
      "loen\n",
      "globoorst\n",
      "baenhoikerveong\n",
      "hueveilem\n",
      "doelgenulezerk\n",
      "Step 20000, Loss= 1.8061\n",
      "list of sampled characters:\n",
      "kroogbouxox palk\n",
      "brou\n",
      "louvel\n",
      "groefam\n",
      "hat brawhisent\n",
      "doxalgowill\n",
      "bamontean\n",
      "eie\n",
      "broontst\n",
      "elsdenwechtoyn\n",
      "Step 22000, Loss= 2.5251\n",
      "list of sampled characters:\n",
      "bekgi awam\n",
      "ak\n",
      "erp\n",
      "linarg\n",
      "ijk\n",
      "due\n",
      "li sgomp\n",
      "di bi\n",
      "hink\n",
      "hinchtoep\n",
      "Step 24000, Loss= 2.4710\n",
      "list of sampled characters:\n",
      "heuiee\n",
      "eukenven lin\n",
      "coudeu\n",
      "bygloed\n",
      "uibi louken\n",
      "haltachser\n",
      "luseragel\n",
      "enen\n",
      "keyl\n",
      "loerndouke\n",
      "Step 26000, Loss= 2.2351\n",
      "list of sampled characters:\n",
      "gen\n",
      "gaskberden\n",
      "dejnmerilzreaide\n",
      "de aden\n",
      "annli-etdensdender\n",
      "kuorbenerweeve chadooien\n",
      "huekupwes\n",
      "bbenudenker\n",
      "umoozenden\n",
      "higberk\n",
      "Step 28000, Loss= 2.7458\n",
      "list of sampled characters:\n",
      "boagsch\n",
      "boembenhot\n",
      "kuld\n",
      "adsgoen\n",
      "grinhaur\n",
      "broanhuo\n",
      "heolensieboidenvegter\n",
      "gorkpoott\n",
      "eldhche\n",
      "goane\n",
      "Step 30000, Loss= 2.0057\n",
      "list of sampled characters:\n",
      "de aartjee\n",
      "groidautsderr\n",
      "louchen\n",
      "khtermors\n",
      "linwade\n",
      "blooukenaf\n",
      "hil\n",
      "eil\n",
      "laeantsboli\n",
      "geceeg\n",
      "Step 32000, Loss= 2.4476\n",
      "list of sampled characters:\n",
      "lijlinbrool\n",
      "egesden\n",
      "keoden\n",
      "hui woik\n",
      "grouk\n",
      "ool\n",
      "helkentend\n",
      "felhoozer\n",
      "etien\n",
      "de donbeal\n",
      "Step 34000, Loss= 1.9942\n",
      "list of sampled characters:\n",
      "hoeesghoy\n",
      "hiek\n",
      "heup\n",
      "hilel\n",
      "loilhenel\n",
      "kout\n",
      "broay\n",
      "algusnnerp\n",
      "brirdornchon\n",
      "gaegooonup\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x10535ecf8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“DeepLearning”",
   "language": "python",
   "name": "dlr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
